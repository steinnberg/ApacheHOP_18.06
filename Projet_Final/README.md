# ğŸš– ETL + Big Data avec Apache Hop (DistribuÃ©)
Programme pÃ©dagogique complet basÃ© sur le dataset **NYC Taxi Trips**, conÃ§u pour rÃ©aliser:
- lâ€™ETL moderne,
- le Big Data distribuÃ©,
- l'orchestration,
- et lâ€™usage dâ€™Apache Hop avec Spark.

---

## ğŸ—‚ï¸ Plan Global du Projet final

La Roadmap est divisÃ©e en **5 modules**, progressifs et orientÃ©s pratique.  

---

# 1ï¸âƒ£ Module 1 â€” DÃ©couverte du dataset & Pipeline ETL local

### ğŸ¯ Objectifs
- Comprendre la structure du dataset NYC Taxi.
- Prendre en main Apache Hop.
- Construire un premier pipeline ETL local.

### ğŸ“Œ Contenu
- PrÃ©sentation des fichiers Yellow Cab / Green Cab.
- Analyse des colonnes : dates, montants, distances, gÃ©olocations.
- TÃ©lÃ©chargement dâ€™un Ã©chantillon (janvier).
- Pipeline Hop :
  - Extraction CSV/Parquet  
  - Nettoyage (valeurs aberrantes, types, normalisation)
  - Chargement vers CSV/Parquet ou DB locale (DuckDB/PostgreSQL)

### ğŸ Livrable
Pipeline Hop complet + donnÃ©es nettoyÃ©es.

---

# 2ï¸âƒ£ Module 2 â€” Data Warehouse & Partitionnement

### ğŸ¯ Objectifs
- Apprendre les bonnes pratiques de structuration de donnÃ©es.
- Construire un mini Data Warehouse (modÃ¨le en Ã©toile).
- PrÃ©parer la montÃ©e en volume.

### ğŸ“Œ Contenu
- Partitionnement par mois / annÃ©e / zone gÃ©ographique.
- CrÃ©ation des dimensions :
  - DimDate
  - DimZone
  - DimVendor
- CrÃ©ation de la table de faits FactTrips
- ETL Hop :
  - CrÃ©ation des tables dim + fact
  - Jointures
  - KPIs : revenu moyen, distance moyenne, tips, heatmaps

### ğŸ Livrable
Data Warehouse complet (fichiers ou DB) + pipeline Hop.

---

# 3ï¸âƒ£ Module 3 â€” Passage au Big Data : Hop + Spark distribuÃ©

### ğŸ¯ Objectifs
- ExÃ©cuter des pipelines en mode distribuÃ©.
- Manipuler plusieurs gigas de donnÃ©es en cluster.
- IntÃ©grer Hop avec Spark.

### ğŸ“Œ Contenu
- Installation cluster Spark standalone (1 master + 2 workers)
- Configuration Hop pour Spark
- Optimisation mÃ©moire & exÃ©cuteurs
- Pipeline Spark distribuÃ© :
  - Ingestion de lâ€™annÃ©e complÃ¨te
  - Nettoyage distribuÃ©
  - AgrÃ©gations massives (zones, heures, revenus, tips)
  - Jointures avec dimensions

### ğŸ Livrable
Pipeline Hop distribuÃ© fonctionnel (Spark).

---

# 4ï¸âƒ£ Module 4 â€” Orchestration & Monitoring (en option)

### ğŸ¯ Objectifs
- Orchestrer un pipeline Big Data de bout en bout.
- Ajouter du monitoring et des logs.
- Automatiser les tÃ¢ches.

### ğŸ“Œ Contenu
- Workflow Hop : sÃ©quence, dÃ©pendances, erreurs
- ExÃ©cution conditionnelle
- Monitoring Hop Web
- Logging JSON + visualisation
- ExÃ©cution via CLI / scripts Python
- Planification (CRON-like)

### ğŸ Livrable
Workflow complet + monitoring + logs.

---

# 5ï¸âƒ£ Module 5 â€” Projet Final : ETL Big Data distribuÃ© complet

### ğŸ¯ Objectifs
CrÃ©er une architecture Big Data de niveau professionnel.

### ğŸ“Œ Exigences du projet
1. **Pipeline Spark distribuÃ©** :
   - ingestion de lâ€™annÃ©e complÃ¨te NYC Taxi
   - nettoyage massif
   - crÃ©ation du Data Warehouse
   - calcul dâ€™indicateurs avancÃ©s

2. **Orchestration complÃ¨te** :
   - workflow Hop
   - logs + alertes
   - versions & documentation

3. **Sorties attendues** :
   - warehouse final (parquet + SQL)
   - dashboard (PowerBI, Grafana, Streamlit, Superset) (en option)
   - rapport technique + diagrammes des pipelines

### ğŸ Livrable
Un mini systÃ¨me ETL Big Data opÃ©rationnel.

---

## ğŸ“Œ Bonus : IdÃ©es de sujets de projet
- Anomalies de prix / distances (fraude ou erreurs)
- Heatmap des pickups par heure
- Analyse de performance : 1 worker vs 2 vs 4
- Impact de la mÃ©tÃ©o (en ajoutant NOAA)

---

## ğŸ“š Technologies utilisÃ©es
- **Apache Hop**
- **Apache Spark** (standalone, 2â€“4 workers)
- **DuckDB / PostgreSQL**
- **Docker Compose**
- **Grafana / Superset** (option)
- **Python pour orchestration**

---

## âœ¨ Auteur & Contact
Cours prÃ©parÃ© par **Kheireddin Kadri**, Data Scientist & Enseignant-Chercheur.  
Pour toute question : merci dâ€™ouvrir une *issue* dans ce dÃ©pÃ´t.

